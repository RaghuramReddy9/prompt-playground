# Prompt Engineering Playground with Gemini

This project explores how different types of prompts affect the quality of LLM responses using Google Gemini.

We used Gemini 1.5 Flash to generate and evaluate outputs from:
- Direct Prompts
- Role-Based Prompts
- Chain-of-Thought (CoT) Prompts
- LangChain PromptTemplate-based Sentiment Classification
- Multi-Step Prompt Chaining Agent

Each output was optionally scored by:
- Gemini itself (self-evaluation)
- Manual scoring logic (1 = correct, 0 = incorrect)

---

## Project Goals

- Compare multiple prompt styles using real-world examples.
- Automate evaluation scoring with LLMs.
- Build a reusable playground for testing prompt engineering ideas.
- Test prompt templates with LangChain and analyze response accuracy.
- Chain LLM prompts together for multi-stage reasoning tasks.

---

## Project Structure

prompt-playground/
â”œâ”€â”€ .venv/ â† Python virtual environment
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ prompt_playground.ipynb
â”‚
â”œâ”€â”€ prompts/
â”‚ â”œâ”€â”€ cot.txt
â”‚ â”œâ”€â”€ direct.txt
â”‚ â”œâ”€â”€ few_shot.txt
â”‚ â”œâ”€â”€ role_based.txt
â”‚ â”œâ”€â”€ scorer.txt
â”‚ â”œâ”€â”€ sentiment_template.txt
â”‚ â”œâ”€â”€ chain_step1_extract.txt
â”‚ â”œâ”€â”€ chain_step2_formalize.txt
â”‚ â””â”€â”€ chain_step3_decide.txt
â”‚
â”œâ”€â”€ results/
â”‚ â””â”€â”€ sentiment_scores.csv
â”‚
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ generate_and_score.py
â”‚ â”œâ”€â”€ generate_from_template.py
â”‚ â””â”€â”€ chained_review_agent.py
â”‚
â”œâ”€â”€ screenshots/
â”‚ â”œâ”€â”€ prompt_outputs.png
â”‚ â””â”€â”€ scoring.png
â”‚
â””â”€â”€ .env â† API keys (not pushed to GitHub)


---

## ğŸ†• Module: Sentiment Classification with LangChain + Gemini

This script demonstrates how to:
- Use `LangChain.PromptTemplate` to fill a structured prompt
- Send the review to Gemini Flash (1.5)
- Get predicted sentiment (Positive/Negative)
- Compare it to expected label
- Score accuracy (1 = correct, 0 = incorrect)
- Save all outputs to `sentiment_scores.csv`

### Example Output

```text
--- Review 1 ---
Input: The movie was fantastic and had great acting.
Sentiment: Positive
Expected: Positive
Score: 1

 Total Score: 3/3
 Accuracy: 100.00%
```
| Review                                             | Expected | Predicted | Score |
| -------------------------------------------------- | -------- | --------- | ----- |
| The movie was fantastic and had great acting.      | Positive | Positive  | 1     |
| It was boring and I almost fell asleep.            | Negative | Negative  | 1     |
| The visuals were good, but the plot made no sense. | Negative | Negative  | 1     |

â–¶ï¸ How to Run
 "python scripts/generate_from_template.py"

---

## New Module: Chained Gemini Agent (3-Step Reasoning)

This module simulates a real AI assistant that reads a customer review and:

1. Summarizes the core issue
2. Rewrites the summary in a formal tone
3. Recommends one action: ["Flag", "Escalate", "Thank", "Ignore"]

---

### Prompt Chain

| Step | Prompt Template               | Role              |
|------|-------------------------------|-------------------|
| 1ï¸âƒ£   | chain_step1_extract.txt       | Extract Sentiment |
| 2ï¸âƒ£   | chain_step2_formalize.txt     | Formal Rewrite    |
| 3ï¸âƒ£   | chain_step3_decide.txt        | Recommend Action  |

---

### Sample Output

```text
Step 1 - Summary: Damaged product upon arrival (scratched screen).
Step 2 - Formal Report: Upon arrival, the product exhibited damage in the form of screen scratches.
Step 3 - Recommended Action: Escalate
```
---

## ğŸ“¸ Screenshots

### Prompt Outputs (Multi-Prompt Evaluation)
![Prompt Outputs](screenshots/prompt_outputs.png)

### Evaluation Score Table
![Scoring Output](screenshots/scoring.png)

### Chained Agent Output
![Chained Agent Output](screenshots/chained_agent_output.png)

# Prompt Engineering Playground with Gemini

This project explores how different types of prompts affect the quality of LLM responses using Google Gemini.

We used Gemini 1.5 Flash to generate and evaluate outputs from:
- Direct Prompts
- Role-Based Prompts
- Chain-of-Thought (CoT) Prompts
- LangChain PromptTemplate-based Sentiment Classification

Each output was optionally scored by:
- Gemini itself (self-evaluation)
- Manual scoring logic (1 = correct, 0 = incorrect)

---

## Project Goals

- Compare multiple prompt styles using real-world examples.
- Automate evaluation scoring with LLMs.
- Build a reusable playground for testing prompt engineering ideas.
- Test prompt templates with LangChain and analyze response accuracy.

---

## ğŸ“ Project Structure
prompt-playground/
â”‚
â”œâ”€â”€ .venv/ â† Python virtual environment
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ prompt_playground.ipynb
â”‚
â”œâ”€â”€ prompts/
â”‚ â”œâ”€â”€ cot.txt
â”‚ â”œâ”€â”€ direct.txt
â”‚ â”œâ”€â”€ few_shot.txt
â”‚ â”œâ”€â”€ role_based.txt
â”‚ â”œâ”€â”€ scorer.txt
â”‚ â””â”€â”€ sentiment_template.txt 
â”‚
â”œâ”€â”€ results/
â”‚ â””â”€â”€ sentiment_scores.csv 
â”‚
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ generate_and_score.py
â”‚ â””â”€â”€ generate_from_template.py 
â”‚
â”œâ”€â”€ screenshots/
â”‚ â”œâ”€â”€ prompt_outputs.png
â”‚ â””â”€â”€ scoring.png
â”‚
â””â”€â”€ .env â† API keys (not pushed to GitHub)

---


## ğŸ“¸ Screenshots

### ğŸ–¼ï¸ Prompt Outputs Example  
![Prompt Outputs](screenshots/prompt_outputs.png)

### ğŸ“Š Scoring Table  
![Scoring Output](screenshots/scoring.png)


---


## ğŸ†• New Module: Sentiment Classification with LangChain + Gemini

This script demonstrates how to:
- Use `LangChain.PromptTemplate` to fill a structured prompt
- Send the review to Gemini Flash (1.5)
- Get predicted sentiment (Positive/Negative)
- Compare it to expected label
- Score accuracy (1 = correct, 0 = incorrect)
- Save all outputs to `sentiment_scores.csv`

---

### ğŸ§ª Example Output

```markdown
```text
--- Review 1 ---
Input: The movie was fantastic and had great acting.
Sentiment: Positive
Expected: Positive
Score: 1

âœ… Total Score: 3/3
âœ… Accuracy: 100.00%

| Review                                             | Expected | Predicted | Score |
| -------------------------------------------------- | -------- | --------- | ----- |
| The movie was fantastic and had great acting.      | Positive | Positive  | 1     |
| It was boring and I almost fell asleep.            | Negative | Negative  | 1     |
| The visuals were good, but the plot made no sense. | Negative | Negative  | 1     |


# Set up project as usual...
# Run the sentiment classification scoring script
python scripts/generate_from_template.py

----
